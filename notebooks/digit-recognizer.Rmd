---
title: "Digit Recognizer from Kaggle"
output: html_document
---

## 1. Introduction

### Project

In collaboration with some friends, and heavily relying on existing resources, this is my process to solve the Digit Recognizer Kaggle competition in R. 

For all details about this competition see the [Digit Recognizer Kaggle competition webpage](https://www.kaggle.com/c/digit-recognizer/data).

### Data

The Kaggle competition is based on the [MNIST database](http://yann.lecun.com/exdb/mnist/) (Modified National Institute of Standards and Technology database), a database of handwritten digits with a training set of 60,000 examples and a test set of 10,000 examples. The original MNIST dataset therefore has a total of 70,000 normalized and centered 28x28 images (28 pixels in height and 28 pixels in width, for a total of 784 pixels).

The format of the datasets provided for the Kaggle competition is different from the original format. First, instead of 28x28 pixel images, Kaggle provided the data reshaped to a single line for each image from the initial 28x28 pixel images. Each pixel is represented by a single value ranging between 0 and 255, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. Second, the training set contains 42,000 examples as opposed to the 60,000 images in the original dataset. For the competition, we have 42,000 examples to train a model to predict 28,000 labels. 

Kaggle provided three csv files with the following descriptions: 

* train.csv - the training dataset with 785 columns. The first column, called "label", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image. Each pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).

* test.csv - same as the training set, except that it does not contain the "label" column.

* sample-submission.csv - for each of the 28,000 images in the test set, output a single line containing the ImageId and the digit you predict.

### Evaluation

The evaluation metric for this contest is the categorization accuracy, or the proportion of test images that are correctly classified. For example, a categorization accuracy of 0.97 indicates that you have correctly classified all but 3% of the images.
  
# 2. Approach

This is a single label multi-class (10 options, one for each digit from 0 to 9) classification (trying to predict a categorical variable) problem, in other words, a computer vision problem for which we can use a neural networks, and even better, a convolutional neural network.

One could use the 42,000 examples to train a model and then apply this to test.csv, submit to the Kaggle website and get the accuracy ranking. However, in order to be able to predict the accuracy before submitting the data to Kaggle, we need to split the training data (which are labeled) into training and testing datasets. To do so, one can split the data 75/25 for training and testing.  

Based on the Deep Learning with R in Motion: the [MNIST dataset video](https://www.youtube.com/watch?v=K6e8WnJeivQ) from Manning publications and the [Deep Learning with R book](https://www.manning.com/books/deep-learning-with-r) written by Francois Chollet and H.H. Allaire, I took the following approach and made the following notes. For this project, I therefore included the following steps:
a. Data preparation
b. Model definition
c. Evaluation
d. Submission

### a. Data preparation

#### Set-up and data
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) # displays code in output document
rm(list = ls()) # clears all objects from the workspace
```

The following packages were used for this project: tidymodels for splitting the dataset in training and testing, keras to run the convnet and tidyverse.
```{r packages, include = TRUE}
library(tidymodels)
library(tidyverse)
library(keras)
```

```{r data, include = TRUE}
labeled <- read.csv("~/projects/kaggle/digit-recognizer/data/raw/train.csv")
unlabeled <- read.csv("~/projects/kaggle/digit-recognizer/data/raw/test.csv")
```

```{r, include = TRUE}
dim(labeled)
dim(unlabeled)
```

```{r, include = TRUE}
labeled[1:6,1:4] # prints first 6 rows and 4 columns
```

```{r, include = TRUE}
unlabeled[1:6,1:4] # prints first 6 rows and 4 columns
```

We see from the above that the labeled dataset includes 42,000 rows (so 42,000 images) and 785 columns ( one column for the label and 784 columns for each pixel). The unlabeled dataset is the same as the labeled one however only includes 28,000 rows for which the label has to be predicted, hence, the labels are not included.

#### Split the training dataset

Good practice is to take the dataset on which training will be done and to split it in two datasets: one for training and another for testing. This can be done manually, with "sort(sample(nrow(unlabeled), nrow(unlabeled)* 0.75))" or one can use the tidymodels package to do so as done below. 

```{r, include = TRUE}
set.seed(1234)
split <- initial_split(labeled)
train <- training(split)
test <- testing(split)

dim(train)
dim(test)
```
We see from above that we took the labeled data (42,000) and splited it training (75% so 31,500 images) and testing (25% so 10,500) datasets.

#### Normalize the values

The next step is to normalize the values so that each pixel is represented by a value between 0 and 1 (as opposed to 0 and 255). This is done by simply dividing the value of each pixel by 255.

```{r}
train_features <- train[,2:785]/255
train_labels <- train[,1]

test_features <- test[,2:785]/255
test_labels <- test[,1]

unlabeled_features <- unlabeled/255
```

#### Reshape the data

The data has to be in a matrix of a specific dimension.
```{r}
# first change data.frames to matrices:
train_features <- data.matrix(train_features)
train_labels <- data.matrix(train_labels)
test_features <- data.matrix(test_features)
test_labels <- data.matrix(test_labels)
unlabeled_features <- data.matrix(unlabeled_features)

# then, reshape the matrices containing the features in 4 dimensions
# if do not reshape the data will have the following error message: 
# expected conv2d_input to have 4 dimensions, but got array with shape (28000, 784)
train_features <- array_reshape(train_features, c(nrow(train_features), 28, 28, 1))
test_features <- array_reshape(test_features, c(nrow(test_features), 28, 28, 1))
unlabeled_features <- array_reshape(unlabeled_features, c(nrow(unlabeled_features), 28, 28, 1))
```

#### Reformat the labels

Need to format the labels as the data (to categorical), aka one hot encoding.
```{r}
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)
```

### b. Model definition

In this section, I heavily relied on the book [Deep Learning with R book](https://www.manning.com/books/deep-learning-with-r) by Francois Chollet and H.H. Allaire.

#### Instantiating a small convnet

As we already know that we want a convolutional neural network, also known as CNN or convnet. The first step is to instantiate a small convnet.

```{r}
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu", input_shape = c(28, 28, 1)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu")%>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu")
```

We chose to use a convnet simply because they have been successful at computer vision tasks (Chollet and Allaire, 2018). The above step in which a convnet was created is generic. The MNIST dataset has not yet been passed to the convnet at this point. 

Let's unpack the above before moving forward. Note that explanations are taken from Chollet and Allaire (2018). The convnet is built with the keras_model_sequential() function from the keras package. This function builds a [model composed of linear stack of layers](https://www.rdocumentation.org/packages/keras/versions/2.2.5.0/topics/keras_model_sequential) which we can see, in this case, there is a total of 5 layers, conv_2d, max_pooling_2d, conv_2d, max_pooling_2d and conv_2d. Let's unpack this:

##### conv_2d layer

Watching this very useful [video](https://www.youtube.com/watch?v=YRhxdVk_sIs), it became clear to me that the convolution part of the CNN, detects patterns (objects, shapes, textures, edges, corners, circles, etc) in the images through the use of filters. Each filter will detect a different kind of pattern, starting with simple pattern detections (e.g., edge) and progressing to more complex patterns (e.g., eye, ear, feather) at deeper layers to finally be able to detect even more complex objects (e.g., cats, dogs). The size of the filter (refered to as the kernel_size in the model) determines the size of the matrix that will be slidded, or convolved, over the image. I encourage you to watch this video as convnet are really well explained. Thank you to the deeplizard.

So, the convnet_2d_layer requires 3 to 4 parameters, let's unpack those:
* input_shape = c(28, 28, 1): this is passed to the first layer only, in this case, it indicates that the images are 28 x 28 x 1
* filters = 32: the number of filters applied in this layer (32 and 64 seems to be standard)
* kernel_size = c(3, 3): the size of the filter, here 3x3 pixels
* activation = "relu": an activation function typically follows a layer in a neural network to determine the output. See this other [video about activation](https://www.youtube.com/watch?v=m0pIlLfpXWE) for a clear explation. Relu is one of many activation function. The relu activation function will transform all negative input to zero and positive inputs are transformed to positive outputs (the more positive the input, the more activated the neuron is).

##### max_pooling_2d

The max_pooling_2d step is a step to aggresively downsample feature maps Chollet and Allaire (2018). It works in a similar way as the convnet, in the sense that it slides over a given set of pixel, but in this case, returns the highest value. There is a great [video](https://www.youtube.com/watch?v=ZjM_XQa5s6s) on YouTube explaining this in details.

#### Adding a classifier on top of the convnet

Before passing on the model to a dense layer, it has to be flatten from 3D to 1D, so this step was added to the model. Note that the MNIST dataset has not yet been passed to the convnet at this point.

```{r}
model <- model %>%
  layer_flatten() %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 10, activation = "softmax")
```

#### Training the convnet on the images

First, the network-compilation step:
```{r}
model %>% compile(
  optimizer = "rmsprop", 
  loss = "categorical_crossentropy",
  metric = c("accuracy")
)
```

The training of the model will attempt to minimise the loss function. 
```{r}
model %>% fit(
  train_features, train_labels,
  epochs = 5, batch_size = 64
)
```

### c. Evaluation

We then use the trained model to test the accuracy on the test_features and test_labels (the other subset of the labeled dataset).
```{r}
results <- model %>% evaluate(test_features, test_labels)
model
results
```
We can see from the above that the model leads to 0.9873334 accuracy (this is determined on the labeled dataset). This is then to be used in the submission below, to predict another unlabeled dataset, with 28,000 images. 

### d. Submission

Passing the unlabeled normalised features to the model:
```{r predict}
predicted_labels <- model %>% predict_classes(unlabeled_features)
predicted_labels
```

```{r submission format}
# needs to be two columns: ImageID, Label
ImageID <- c(1:28000)
Label <- predicted_labels
df <- data.frame(ImageID, Label)
path <- "~/projects/kaggle/digit-recognizer/data/processed/submission.csv"
write_csv(df, path)
```

When submitted to the Kaggle website, the above lead to a 0.98885 accuracy.

Changing the split from 3/4 to 4/5, in other words using more data to train the model, lead to an accuracy of 0.9871413 against labeled test data and 0.98828 against the unlabeled data (on the Kaggle website - after submission). So training with more data (80% instead of 75%) did not improve the accuracy of the model.

Reverting to 3/4 of data (default) to train model, this time with 15 instead of 5 epochs, lead to an accuracy of 0.988381 against labeled test data and 0.98785 against the unlabeled data (on the Kaggle website - after submission). So training with more epochs did not improve the accuracy of the model.

Changing the split from 3/4 to 19/20, in other words using almost all data to train the model, lead to an accuracy of 0.9880952 against labeled test data and 0.98714 against the unlabeled data (on the Kaggle website - after submission). So training with more data (95% instead of 75%) did not improve the accuracy of the model.

## 3. Conclusion

So the approch (convnet) worked well to predict handwritten digits leading to 0.98885 accuracy on the unlabeled dataset using default settings. Changing the number of epochs or the proportion of the data used to train the model did not lead to a higher accuracy.

## 4. Cited reference

Chollet and Allaire, 2018. Deep Learning with R. Manning. 
